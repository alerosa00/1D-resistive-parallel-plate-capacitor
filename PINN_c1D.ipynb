{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f59b9-e624-49ab-8cea-e83e98c483b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import h5py\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch.nn.init as init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36baf80c-5afa-40e4-9e29-714adde4c7a8",
   "metadata": {},
   "source": [
    "<h3>Geometry</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935a66b7-e08e-4e56-85cc-3c2e2f7b3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# PINN DEFAULT SETTING :\n",
    "PDE_batch_size  = 4096\n",
    "IC_batch_size   = 4096\n",
    "BC_batch_size   = 4096\n",
    "\n",
    "# GEOMETRY\n",
    "\n",
    "# ANALOGO TEMPO TAU DEL METODO PSEUDOSPETTRALE (DOVE HO USATO LE STESSE LUNGHEZZE DEL CONDENSATORE)\n",
    "epsilon = 5.7 * 8.85 * 1e-9   # STESSO epsilon usato nel metodo pseudospettrale\n",
    "lato, L, d, V0 = 1e3, 1e2, 20, 1\n",
    "tau = 2.144                   # STESSO tau usato nel metodo pseudospettrale\n",
    "tau_pde = tau\n",
    "_sigma = epsilon / tau\n",
    "\n",
    "# la pde sotto è SOLO in z e t\n",
    "x_min = -lato/2\n",
    "x_max = +lato/2\n",
    "y_min = -lato/2\n",
    "y_max = +lato/2\n",
    "z_min = -L/2\n",
    "z_max = +L/2\n",
    "factor_x, factor_y, factor_z = x_max-x_min, y_max-y_min, z_max-z_min\n",
    "\n",
    "t_min = 0\n",
    "t_max = 5 * tau              # almost complete charge of capacitor\n",
    "factor_t = (t_max-t_min)\n",
    "c_time_err = 1e-2\n",
    "logt_min = np.log( t_min + c_time_err )\n",
    "logt_max = np.log( t_max + c_time_err )\n",
    "factor_logt = logt_max-logt_min\n",
    "\n",
    "\n",
    "print(f\"logt_min, logt_max : {logt_min}, {logt_max}\")\n",
    "print(f\"tau : {tau} - epsilon : {epsilon} - sigma : {_sigma}\")\n",
    "\n",
    "\n",
    "V0 = 1\n",
    "E0 = V0 / ( z_max - z_min )\n",
    "\n",
    "\n",
    "flag_norm_log = True\n",
    "\n",
    "if flag_norm_log == True:    # with log-transform\n",
    "    # NORMALIZATION of TIME\n",
    "    def Norm_time(time_coords):\n",
    "        if isinstance(time_coords, torch.Tensor):\n",
    "            return ( torch.log( time_coords + c_time_err ) ) / factor_logt\n",
    "        else:\n",
    "            return ( np.log( time_coords + c_time_err ) ) / factor_logt\n",
    "    # INVERSE OF NORMALIZATION of TIME\n",
    "    def Inv_Norm_time(norm_time_coords):\n",
    "        if isinstance(norm_time_coords, torch.Tensor):\n",
    "            return ( torch.exp( norm_time_coords * factor_logt ) - c_time_err )\n",
    "        else:\n",
    "            return ( np.exp( norm_time_coords * factor_logt  ) - c_time_err )\n",
    "\n",
    "else:     # with linear-transform\n",
    "    def Norm_time(time_coords):\n",
    "        if isinstance(time_coords, torch.Tensor):\n",
    "            return (time_coords) / (factor_t)\n",
    "        else:\n",
    "            return (time_coords) / (factor_t)\n",
    "    \n",
    "    def Inv_Norm_time(norm_time_coords):\n",
    "        if isinstance(norm_time_coords, torch.Tensor):\n",
    "            return norm_time_coords * (factor_t)\n",
    "        else:\n",
    "            return norm_time_coords * (factor_t)\n",
    "\n",
    "# NORMALIZATION OF SPACE\n",
    "def Norm_z(z_coord):\n",
    "    return ( z_coord - z_min ) / factor_z\n",
    "        \n",
    "def Inv_Norm_z(norm_z_coord):\n",
    "    return norm_z_coord * factor_z + z_min\n",
    "\n",
    "\n",
    "def Norm_x(x_coord):\n",
    "    return ( x_coord ) / factor_x\n",
    "def Inv_Norm_x(norm_x_coord):\n",
    "    return norm_x_coord * factor_x\n",
    "def Norm_y(y_coord):\n",
    "    return ( y_coord ) / factor_y\n",
    "def Inv_Norm_y(norm_y_coord):\n",
    "    return norm_y_coord * factor_y\n",
    "\n",
    "\n",
    "norm_t_min, norm_x_min, norm_y_min, norm_z_min = Norm_time(t_min), Norm_x(x_min), Norm_y(y_min), Norm_z(z_min)\n",
    "norm_t_max, norm_x_max, norm_y_max, norm_z_max = Norm_time(t_max), Norm_x(x_max), Norm_y(y_max), Norm_z(z_max)\n",
    "print(\"normalized coordinates:\")\n",
    "print(f\"REAL =>  time : [{t_min},{t_max}], space : [{x_min},{x_max}]x[{y_min},{y_max}]x[{z_min},{z_max}]\")\n",
    "print(f\"NORM =>  time : [{norm_t_min},{norm_t_max}], space : [{norm_x_min},{norm_x_max}]x[{norm_y_min},{norm_y_max}]x[{norm_z_min},{norm_z_max}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f22c31-c9da-4b3f-ad70-dc86235d4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# verifica che le conversioni diano l'identità sia rispetto al tempo che spazio\n",
    "print(\"z\")\n",
    "VVV = torch.rand((1,6), device=device)\n",
    "print(VVV - Norm_z(Inv_Norm_z(VVV)))\n",
    "print(VVV - Inv_Norm_z(Norm_z(VVV)))\n",
    "print(VVV)\n",
    "print(Norm_z(Inv_Norm_z(VVV)))\n",
    "print(Inv_Norm_z(Norm_z(VVV)))\n",
    "print(\"t\")\n",
    "VVV = torch.rand((1,6), device=device)\n",
    "print(VVV - Norm_time(Inv_Norm_time(VVV)))\n",
    "print(VVV - Inv_Norm_time(Norm_time(VVV)))\n",
    "print(VVV)\n",
    "print(Norm_time(Inv_Norm_time(VVV)))\n",
    "print(Inv_Norm_time(Norm_time(VVV)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88684f9d-0987-484c-bb64-d3ff43e8c311",
   "metadata": {},
   "source": [
    "<h1>Sigma ML</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552a3876-92b8-4541-b1e8-38c184a665d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_train_sigma = True    # se == True, allora allena la neural network della sigma; \n",
    "                           # se == False, la prende dal file \"sigma_model_plane1D.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824a608b-243e-42c1-ae6a-4a416d84a2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigmoid(value):\n",
    "    # SMOOTH :\n",
    "    #mu = factor_z/2 * 2.5\n",
    "    #sigmoid = 1 / ( 1 + torch.exp( 14 - mu * value ))\n",
    "    # NON SMOOTH :\n",
    "    mu = factor_z/2 * 3.4\n",
    "    sigmoid = 1 / ( 1 + torch.exp( 17 - mu * value ))\n",
    "    return sigmoid\n",
    "\n",
    "def generate_sigma_coords(n_points=PDE_batch_size):\n",
    "    t_coords = norm_t_min + (norm_t_max - norm_t_min) * torch.rand(n_points, 1, device=device)\n",
    "    z_coords = norm_z_min + (norm_z_max - norm_z_min) * torch.rand(n_points, 1, device=device)\n",
    "    points = torch.cat( [t_coords, z_coords], dim=-1 )\n",
    "    mask = (points[:,1] > Norm_z( z_max-L/2 )) | (points[:,1] < Norm_z( z_min+L/2 ))\n",
    "    sigma_coords = torch.zeros_like(points[:,0].unsqueeze(-1), device=device)\n",
    "    sigma_coords[mask] = torch.ones_like(sigma_coords[mask], device=device)\n",
    "    sigma_coords = sigma_coords * get_sigmoid( torch.abs(points[:,1].unsqueeze(-1) - Norm_z( z_max-L/2 )) ) \\\n",
    "                                * get_sigmoid( torch.abs(points[:,1].unsqueeze(-1) - Norm_z( z_min+L/2 )) )\n",
    "    return points, sigma_coords\n",
    "\n",
    "\n",
    "if flag_train_sigma == True:\n",
    "    # PLOTs\n",
    "    points, sigma_coords = generate_sigma_coords(10000)\n",
    "    # plane XZ\n",
    "    z_coords = Inv_Norm_z( points[:,1] )\n",
    "    z_coords = z_coords.detach().cpu().numpy()\n",
    "    sigma_coords = sigma_coords.detach().cpu().numpy()    \n",
    "    plt.scatter( z_coords, sigma_coords, c=\"green\", s=1 )\n",
    "    plt.axvline(x=-10, color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "    plt.axvline(x=10,  color=\"red\", linestyle=\"--\", linewidth=1.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b710ad-a4ae-44de-b627-e966f32aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### SIGMA DNN model\n",
    "# è una neural network normale con ML classico (prende i dati da generate_sigma_coords(...) sopra)\n",
    "\n",
    "class DNN_model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, n_nodes, n_layers, n_batches, dropout):\n",
    "        super().__init__()  \n",
    "        # DNN part\n",
    "        self.n_batches = n_batches\n",
    "        self.dropout = dropout\n",
    "        layers = [nn.Linear(input_dim, n_nodes), nn.Tanh(), nn.Dropout(self.dropout)]\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(self.dropout))\n",
    "        layers.append(nn.Linear(n_nodes, 1))\n",
    "        layers.append(nn.Tanh())\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "  \n",
    "    def train_model(self, optimizer, loss_function, epochs=500, batch_size=4096, validation_split=0.1):\n",
    "        print(\"Training in progress...\")\n",
    "        start_time = time.time()\n",
    "        n_train_batches = int( (1 - validation_split) * self.n_batches)\n",
    "        n_val_batches   = self.n_batches - n_train_batches\n",
    "        print(f\"  number of epochs             : {epochs}\")\n",
    "        print(f\"  number of train batches      : {n_train_batches}\")\n",
    "        print(f\"  number of validation batches : {n_val_batches}\")\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "        history = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            # === TRAINING STEP ===\n",
    "            self.train()\n",
    "            start_data_tr = time.time()\n",
    "            data_total_loss = 0            \n",
    "            for indx_batch in range(n_train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                data_coords, target = generate_sigma_coords(batch_size)\n",
    "                data_coords.requires_grad=True\n",
    "                predictions = self.forward(data_coords)\n",
    "                loss = loss_function( predictions, target )\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                data_total_loss += loss.item()\n",
    "            data_total_loss /= n_train_batches\n",
    "            end_data_tr = time.time()\n",
    "\n",
    "            # === VALIDATION STEP ===\n",
    "            self.eval()\n",
    "            start_val = time.time()\n",
    "            data_val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for indx_batch in range(n_val_batches):\n",
    "                    optimizer.zero_grad()\n",
    "                    data_coords, target = generate_sigma_coords(batch_size)\n",
    "                    data_coords.requires_grad=True\n",
    "                    predictions = self.forward(data_coords)\n",
    "                    loss = loss_function( predictions, target )\n",
    "                    data_val_loss += loss.item()\n",
    "                data_val_loss /= n_val_batches\n",
    "            end_val = time.time()\n",
    "            scheduler.step(data_val_loss)\n",
    "\n",
    "            end = time.time()\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - data loss: {data_total_loss} - val Loss: {data_val_loss}\")\n",
    "            print(f\"    time : {end - start} s  -  data: {end_data_tr - start_data_tr} s - val: {end_val - start_val} s\")\n",
    "            print(f\" \")\n",
    "            history.append([ data_total_loss, data_val_loss ])\n",
    "            if data_total_loss < 1e-7:\n",
    "                # save model\n",
    "                torch.save(self.state_dict(), \"sigma_model_plane1D.pth\")\n",
    "                print(f\"Done!  (time : {time.time() - start_time})\")\n",
    "                return history\n",
    "\n",
    "        # save model\n",
    "        torch.save(self.state_dict(), \"sigma_model_plane1D.pth\")\n",
    "        print(f\"Done!  (time : {time.time() - start_time})\")\n",
    "        return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9905710-b90a-4740-af2d-96e31d72ba15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# construction and training of DNN SIGMA\n",
    "sigma_model = 0\n",
    "input_dim = 2     # (t, z)\n",
    "n_nodes = 64\n",
    "n_layers = 5\n",
    "n_batches = 96\n",
    "dropout = 0\n",
    "sigma_model = DNN_model( input_dim, n_nodes, n_layers, n_batches, dropout ).to(device)\n",
    "\n",
    "if flag_train_sigma == True:\n",
    "    optimizer = torch.optim.Adam(sigma_model.parameters(), lr=0.001, weight_decay=1e-8)\n",
    "    loss_function = nn.MSELoss()\n",
    "    history_sigma = sigma_model.train_model(optimizer, loss_function, epochs=400)\n",
    "    print(sigma_model)\n",
    "    \n",
    "    # history of training\n",
    "    plt.plot([pair[0] for pair in history_sigma], label=\"Training\")\n",
    "    plt.plot([pair[1] for pair in history_sigma], label=\"Validation\")\n",
    "    plt.legend(title=\"Error\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"MSE\")\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.yscale(\"log\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cff98d1-3131-4f65-8b3b-845a729392d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if flag_train_sigma == False:  # se non ho allenato la sigma, allora prende i pesi che avevo salvato dal file\n",
    "    sigma_model.load_state_dict(torch.load(\"sigma_model_plane1D.pth\"))\n",
    "\n",
    "# PLOTS\n",
    "sampled_times = np.linspace(Norm_time(t_min), Norm_time(t_max), 6)\n",
    "n_samples = 100000\n",
    "plt.figure(figsize=(16,10))\n",
    "sigma_model.eval()\n",
    "\n",
    "for i_plot, n_t in enumerate(sampled_times, 1):\n",
    "    real_t = Inv_Norm_time(n_t)\n",
    "    X_pred, sigma_true = generate_sigma_coords(n_samples)\n",
    "    X_pred[:,0] = n_t * torch.ones_like( X_pred[:,0], device=device )\n",
    "    X = X_pred.detach().cpu().numpy()\n",
    "    # predicted result\n",
    "    with torch.no_grad():\n",
    "        sigma_pred = sigma_model.forward(X_pred)\n",
    "    sigma_pred = sigma_pred.detach().cpu().numpy()\n",
    "    # true result\n",
    "    sigma_true = sigma_true.detach().cpu().numpy()\n",
    "    \n",
    "    plt.subplot(3, len(sampled_times), i_plot)\n",
    "    plt.title(f\"Pred {real_t:3f} ns\")\n",
    "    im0=plt.scatter(Inv_Norm_z(X[:, 1]), sigma_pred, s=2)\n",
    "    plt.colorbar(im0)\n",
    "    \n",
    "    plt.subplot(3, len(sampled_times), len(sampled_times) + i_plot)\n",
    "    plt.title(f\"True {real_t:3f} ns\")\n",
    "    im1=plt.scatter(Inv_Norm_z(X[:, 1]), sigma_true, s=2)\n",
    "    plt.colorbar(im1)\n",
    "\n",
    "    plt.subplot(3, len(sampled_times), 2*len(sampled_times) + i_plot)\n",
    "    plt.title(f\"Error {real_t:3f} ns\")\n",
    "    im2=plt.scatter(Inv_Norm_z(X[:, 1]), np.abs(sigma_true-sigma_pred), s=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "sigma_model.eval()\n",
    "n_points = 10000\n",
    "t_coords = norm_t_min * torch.ones(n_points, 1, device=device)\n",
    "z_coords = norm_z_min + (norm_z_max - norm_z_min) * torch.rand(n_points, 1, device=device)\n",
    "coords_sample = torch.cat( [t_coords, z_coords], dim=-1 )\n",
    "sigma_sample  = sigma_model( coords_sample )\n",
    "z_coords = z_coords.detach().cpu().numpy()\n",
    "sigma_sample = sigma_sample.detach().cpu().numpy()\n",
    "plt.scatter( Inv_Norm_z(z_coords), sigma_sample, c=\"green\", s=1 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549aa6d2-6022-4c0e-8d4b-69f01dd8aa80",
   "metadata": {},
   "source": [
    "<h1>PINN</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f91aad-d986-47b8-b937-fc2f12f29b4e",
   "metadata": {},
   "source": [
    "<h3>Initial conditions</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df49d53e-44b4-4659-8c71-645de46c70da",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_model.eval()\n",
    "\n",
    "def gen_IC_points(model=None, batch_size=IC_batch_size, oversample_factor=4):\n",
    "    # Oversample\n",
    "    M = batch_size * oversample_factor\n",
    "    t_coords = norm_t_min * torch.ones(M, 1, device=device)  # initial time\n",
    "    z_coords = norm_z_min + (norm_z_max - norm_z_min) * torch.rand(M, 1, device=device)\n",
    "    coords = torch.cat([t_coords, z_coords], dim=-1)\n",
    "    z_factor = (z_coords - norm_z_min) / (norm_z_max - norm_z_min)\n",
    "    V_target = V0 * z_factor\n",
    "    # Importance sampling if model is not None\n",
    "    if model is not None:\n",
    "        coords.requires_grad=True\n",
    "        V_pred = model(coords)\n",
    "        grads = torch.autograd.grad(\n",
    "            V_pred, coords,\n",
    "            grad_outputs=torch.ones_like(V_pred),\n",
    "            create_graph=False, retain_graph=False\n",
    "        )[0]\n",
    "        E_pred = ( (grads[:,1].unsqueeze(-1)/factor_x)**2 ) ** (1/2)\n",
    "        weights = torch.abs(V_pred - V_target) + 10*torch.abs(E_pred - E0*torch.ones_like(E_pred)) + 1\n",
    "        weights = torch.clamp(weights.squeeze(-1), min=1e-12)\n",
    "        probs = weights / torch.sum(weights)\n",
    "        # Resampling\n",
    "        idx = torch.multinomial(probs, batch_size, replacement=False)\n",
    "        coords_batch = coords[idx]\n",
    "        values_batch = V_target[idx]\n",
    "    else:\n",
    "        # uniform sampling if model is None\n",
    "        coords_batch = coords[:batch_size]\n",
    "        values_batch = V_target[:batch_size]\n",
    "    #\n",
    "    return coords_batch.detach(), values_batch.detach()\n",
    "    \n",
    "\n",
    "def compute_IC_loss(model, n_points=IC_batch_size, loss_function=nn.MSELoss()):\n",
    "    coords, values = gen_IC_points(model=model, batch_size=n_points)\n",
    "    coords.requires_grad = True\n",
    "    values_pred = model.forward(coords)\n",
    "    loss = loss_function( values_pred, values )\n",
    "    return loss\n",
    "\n",
    "\n",
    "# PLOT IC\n",
    "points, IC_values = gen_IC_points(batch_size=10000)\n",
    "print(f\"initial time : {Inv_Norm_time(np.unique(points[:len(points)//2,0].detach().cpu().numpy()))}\")\n",
    "# plane XZ\n",
    "z_coords = Inv_Norm_z( points[:,1] )\n",
    "z_coords = z_coords.detach().cpu().numpy()\n",
    "IC_values = IC_values.detach().cpu().numpy()\n",
    "fig = plt.scatter(z_coords, IC_values, s=1, c=\"blue\")\n",
    "plt.title(\"Initial condition\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"V\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e072119c-f1a1-40b4-869c-c4077669783c",
   "metadata": {},
   "source": [
    "<h3>Boundary condition</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc96ac4-3cc3-4dc5-9d5c-fa8b6b8a37c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_BC_points(n_points=BC_batch_size):\n",
    "    # minimum z-coord\n",
    "    t_coords = norm_t_min + (norm_t_max - norm_t_min) * torch.rand(n_points, 1, device=device)\n",
    "    z_coords = norm_z_min * torch.ones(n_points, 1, device=device)       \n",
    "    points2 = torch.cat( [t_coords, z_coords], dim=-1 )\n",
    "    values2 = 0 * torch.ones_like( t_coords, device=device )\n",
    "    # maximum z-coord\n",
    "    t_coords = norm_t_min + (norm_t_max - norm_t_min) * torch.rand(n_points, 1, device=device)\n",
    "    z_coords = norm_z_max * torch.ones(n_points, 1, device=device)       \n",
    "    points3 = torch.cat( [t_coords, z_coords], dim=-1 )\n",
    "    values3 = V0 * torch.ones_like( t_coords, device=device )\n",
    "    return torch.cat( [points2, points3], dim=0 ), torch.cat( [values2, values3], dim=0 )\n",
    "\n",
    "def compute_BC_loss(model, n_points=BC_batch_size, loss_function=nn.MSELoss()):\n",
    "    coords, values = gen_BC_points(n_points)\n",
    "    coords.requires_grad = True\n",
    "    values_pred = model.forward(coords)\n",
    "    loss = loss_function( values_pred, values )\n",
    "    return loss\n",
    "\n",
    "# PLOT BC\n",
    "points, BC_values = gen_BC_points(10000)\n",
    "z_coords = Inv_Norm_z( points[:,1] )\n",
    "z_coords = z_coords.detach().cpu().numpy()\n",
    "BC_values = BC_values.detach().cpu().numpy()\n",
    "fig = plt.scatter(z_coords, BC_values, s=3, c=\"blue\")\n",
    "plt.title(\"Boundary condition\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"V\")\n",
    "plt.colorbar(fig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95986bf5-dea9-40de-ae9f-72172e79b8b9",
   "metadata": {},
   "source": [
    "<h3>PDE</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5a229-8a6f-421d-9c47-2dc5210ef832",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps1 = epsilon * 1e8\n",
    "sigma1 = _sigma * 1e8\n",
    "\n",
    "class QSM_PDE:\n",
    "    \n",
    "    def gen_PDE_points(self, model=None, batch_size=PDE_batch_size, oversample_factor=5):\n",
    "        if model is not None:\n",
    "            # Oversample\n",
    "            M = batch_size * oversample_factor\n",
    "            t_coords = norm_t_min + (norm_t_max - norm_t_min) * torch.rand(M, 1, device=device)\n",
    "            z_coords = norm_z_min + (norm_z_max - norm_z_min) * torch.rand(M, 1, device=device)\n",
    "            coords = torch.cat([t_coords, z_coords], dim=-1)\n",
    "            coords.requires_grad=True\n",
    "            V_pred = model(coords)\n",
    "            grads = torch.autograd.grad(\n",
    "                V_pred, coords,\n",
    "                grad_outputs=torch.ones_like(V_pred),\n",
    "                create_graph=False, retain_graph=False\n",
    "            )[0]\n",
    "            dV_dt = grads[:, 0:1]\n",
    "            dV_dx = grads[:, 1:]\n",
    "            grad_norm = torch.norm(dV_dx, dim=1, keepdim=True)\n",
    "            # Importance sampling\n",
    "            weights = ( grad_norm**2 + 5 * dV_dt**2 ) ** 0.5 + torch.abs( V_pred ) + 10\n",
    "            weights = weights.squeeze(-1)\n",
    "            weights = torch.clamp(weights, min=1e-12)  # evitare zero\n",
    "            probs = weights / torch.sum(weights)\n",
    "            # Resampling\n",
    "            idx = torch.multinomial(probs, batch_size, replacement=False)\n",
    "            coords_batch = coords[idx]\n",
    "            return coords_batch.detach()\n",
    "        else:\n",
    "            t_coords = norm_t_min + (norm_t_max - norm_t_min) * torch.rand(n_points, 1, device=device)\n",
    "            z_coords = norm_z_min + (norm_z_max - norm_z_min) * torch.rand(n_points, 1, device=device)\n",
    "            coords = torch.cat([t_coords, z_coords], dim=-1)\n",
    "            return coords\n",
    "        \n",
    "    \n",
    "    def compute_PDE(self, coords, pred_func, flag=False, flag_norm_log=flag_norm_log):\n",
    "        if flag_norm_log == True:\n",
    "            factorrr = torch.exp(coords[:,0].unsqueeze(-1) * factor_logt) * factor_logt\n",
    "        else:\n",
    "            factorrr = factor_t\n",
    "        sigma_model.eval()\n",
    "        sigma_comp = sigma_model.forward(coords)\n",
    "        u = pred_func[:,0]\n",
    "        u_z  = self.get_derivative(u, coords, 1)[:,1].unsqueeze(-1) / factor_z\n",
    "        u_zz = self.get_derivative(u_z, coords, 1)[:,1].unsqueeze(-1) / factor_z\n",
    "        Delta_u = u_zz\n",
    "        Delta_u_t = self.get_derivative(Delta_u, coords, 1)[:,0].unsqueeze(-1)\n",
    "        div_sigma_grad_u = self.get_derivative(sigma_comp, coords, 1)[:,1].unsqueeze(-1) * u_z / factor_z \\\n",
    "                            + sigma_comp * self.get_derivative(u_z, coords, 1)[:,1].unsqueeze(-1) / factor_z\n",
    "        P1 = Delta_u_t / factorrr * eps1\n",
    "        P2 = div_sigma_grad_u * sigma1\n",
    "        eq = P1 + P2\n",
    "        if flag == True:\n",
    "            # se è True stampo ogni parte della pde\n",
    "            u_t = self.get_derivative(u, coords, 1)[:,0] / factorrr\n",
    "            print(f\"u_t in [{(u_t).min()}, {(u_t).max()}] , mean = {torch.mean((u_t))}\")\n",
    "            print(f\"Eps in [{(P1).min()}, {(P1).max()}] , mean = {torch.mean((P1))}\")\n",
    "            print(f\"Div in [{(P2).min()}, {(P2).max()}] , mean = {torch.mean((P2))}\")\n",
    "            print(f\"Eq  in [{(eq).min()}, {(eq).max()}] , mean = {torch.mean((eq))}\")\n",
    "            print(\" \")\n",
    "        return eq.squeeze(-1)*1e3, P1.squeeze(-1)*1e3, P2.squeeze(-1)*1e3\n",
    "        \n",
    "    \n",
    "    def get_derivative(self, y, x, n: int = 1):\n",
    "        if n == 0:\n",
    "            return y\n",
    "        else:\n",
    "            dy_dx = torch.autograd.grad(y, x, torch.ones_like(y).to(y.device), create_graph=True, retain_graph=True, allow_unused=True)[0]              \n",
    "        return self.get_derivative(dy_dx, x, n - 1)\n",
    "\n",
    "\n",
    "    def compute_PDE_loss(self, model, n_points=PDE_batch_size, loss_function=nn.MSELoss(), flag=False, flag_norm_log=True):\n",
    "        coords = self.gen_PDE_points(model=model, batch_size=n_points)\n",
    "        coords.requires_grad = True\n",
    "        values_pred = model.forward(coords)\n",
    "        pde_pred, _, _ = self.compute_PDE(coords, values_pred, flag=flag, flag_norm_log=flag_norm_log)\n",
    "        loss = loss_function( pde_pred, torch.zeros_like(pde_pred, dtype=torch.float32, device=device) )\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b6cc6c-9b13-42f3-b016-304df70a465f",
   "metadata": {},
   "source": [
    "<h3>PINN model</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309c2cc3-2af4-40a3-9638-84eca2b28930",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "############ VERSIONE SENZA SKIP CONNECTION E STAN ACTIVATION FUNCTIONS\n",
    "class PINN_model(nn.Module):\n",
    "    def __init__(self, input_dim, n_nodes, n_layers, n_batches, dropout):\n",
    "        super().__init__()\n",
    "        # PDE part\n",
    "        self._PDE = QSM_PDE()\n",
    "        self.history = []\n",
    "        # DNN part\n",
    "        self.n_batches = n_batches\n",
    "        self.dropout   = dropout\n",
    "        layers = [nn.Linear(input_dim, n_nodes), nn.Tanh(), nn.Dropout(self.dropout)]\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(n_nodes, n_nodes))\n",
    "            layers.append(nn.Tanh())\n",
    "            layers.append(nn.Dropout(self.dropout))\n",
    "        layers.append(nn.Linear(n_nodes, 1))\n",
    "        layers.append(nn.Tanh())\n",
    "        # network\n",
    "        self.network = nn.Sequential(*layers)\n",
    "        # Xavier initialization\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                init.xavier_normal_(layer.weight, gain=1.0)\n",
    "                init.zeros_(layer.bias)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        return self.network(coords)\n",
    "\"\"\"\n",
    "\n",
    "class SelfScaledTanh(nn.Module):\n",
    "    def __init__(self, size, init_beta=0.0):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Parameter(torch.full((size,), init_beta, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.tanh(x) + self.beta * x * torch.tanh(x)\n",
    "    \n",
    "\n",
    "class PINN_model(nn.Module):\n",
    "    def __init__(self, input_dim, n_nodes, n_layers, n_batches, dropout):\n",
    "        super().__init__()\n",
    "        self.n_batches = n_batches\n",
    "        self.dropout = dropout\n",
    "        validation_split = 0.1\n",
    "        self.n_train_batches = int((1 - validation_split) * self.n_batches)\n",
    "        self.n_val_batches = self.n_batches - self.n_train_batches\n",
    "        self.n_layers = n_layers\n",
    "        self.n_nodes = n_nodes\n",
    "        self.history = []\n",
    "        self.optimizer = 0\n",
    "        self._PDE = QSM_PDE()\n",
    "        \n",
    "        # ==== LAYERS ====\n",
    "        self.input_layer = nn.Linear(input_dim, n_nodes)\n",
    "        self.initial_activation = SelfScaledTanh(n_nodes)\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(n_layers - 1):\n",
    "            self.hidden_layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Linear(n_nodes, n_nodes),\n",
    "                    SelfScaledTanh(n_nodes),\n",
    "                    nn.Dropout(self.dropout)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.output_layer = nn.Linear(n_nodes, 1)\n",
    "        self.final_activation = SelfScaledTanh(1)\n",
    "\n",
    "    def forward(self, coords):\n",
    "        x = self.initial_activation(self.input_layer(coords))\n",
    "        residual = x\n",
    "        for i, layer in enumerate(self.hidden_layers):\n",
    "            out = layer(x)\n",
    "            if i % 2 == 1:\n",
    "                x = out + residual\n",
    "                residual = x\n",
    "            else:\n",
    "                x = out\n",
    "        x = self.output_layer(x)\n",
    "        return self.final_activation(x)\n",
    "        \n",
    "    \n",
    "    def train_model(self, optimizer, patience = 10, loss_function = nn.MSELoss(), epochs = 200, validation_split = 0.1):\n",
    "        print(\"Start :\")\n",
    "        start_time = time.time()   \n",
    "        self.history = []\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.95, patience=patience)\n",
    "        # number of batches for training and validation\n",
    "        n_train_batches = self.n_train_batches\n",
    "        n_val_batches   = self.n_val_batches\n",
    "        print(f\"  number of epochs             : {epochs}\")\n",
    "        print(f\"  number of train batches      : {n_train_batches}\")\n",
    "        print(f\"  number of validation batches : {n_val_batches}\")\n",
    "        print(\" \")        \n",
    "        print(\"Training in progress...\")\n",
    "        print(\" \")\n",
    "\n",
    "        weight_IC, weight_BC, weight_PDE = 1, 1, 1\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start = time.time()\n",
    "            \n",
    "            # === TRAINING STEP ===\n",
    "            self.train()\n",
    "            start_tr = time.time()\n",
    "            IC_total_loss, BC_total_loss, PDE_total_loss = 0, 0, 0            \n",
    "            for indx_batch in range(n_train_batches):\n",
    "                optimizer.zero_grad()\n",
    "                IC_loss  = compute_IC_loss(model=self)\n",
    "                BC_loss  = compute_BC_loss(model=self)\n",
    "                if epoch % 50 == 0 and indx_batch == n_train_batches - 1:\n",
    "                    # ogni 50 epoche stampo gli errori di ogni parte della pde\n",
    "                    PDE_loss = self._PDE.compute_PDE_loss(model=self, flag=True )\n",
    "                else:\n",
    "                    PDE_loss = self._PDE.compute_PDE_loss(model=self, flag=False)\n",
    "                Loss = weight_IC * IC_loss + weight_BC * BC_loss + weight_PDE * PDE_loss\n",
    "                Loss.backward()\n",
    "                optimizer.step()\n",
    "                IC_total_loss   += IC_loss.item()\n",
    "                BC_total_loss   += BC_loss.item()\n",
    "                PDE_total_loss  += PDE_loss.item()\n",
    "            IC_total_loss  /= n_train_batches\n",
    "            BC_total_loss  /= n_train_batches\n",
    "            PDE_total_loss /= n_train_batches\n",
    "            end_tr = time.time()\n",
    "\n",
    "            # === VALIDATION STEP ===\n",
    "            self.eval()\n",
    "            start_val = time.time()\n",
    "            IC_val_loss, BC_val_loss, PDE_val_loss = 0, 0, 0  \n",
    "            for indx_batch in range(n_val_batches):\n",
    "                optimizer.zero_grad()\n",
    "                IC_val_loss  += (compute_IC_loss(model=self) ).item()\n",
    "                BC_val_loss  += ( compute_BC_loss(model=self) ).item()\n",
    "                PDE_val_loss += ( self._PDE.compute_PDE_loss(model=self) ).item()\n",
    "            IC_val_loss   /= n_val_batches\n",
    "            BC_val_loss   /= n_val_batches\n",
    "            PDE_val_loss  /= n_val_batches\n",
    "            scheduler.step( weight_IC * IC_val_loss + weight_BC * BC_val_loss + weight_PDE * PDE_val_loss )\n",
    "            end_val = time.time()\n",
    "            \n",
    "            end = time.time()\n",
    "\n",
    "            \n",
    "            lambda_eff = weight_PDE/weight_IC * PDE_total_loss/IC_total_loss\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            # === PRINT LOSSES ===\n",
    "            print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "            print(f\"  Training losses   ==>  IC: {IC_total_loss} - BC: {BC_total_loss} - pde: {PDE_total_loss}\")\n",
    "            print(f\"  Validation losses ==>  IC: {IC_val_loss} - BC: {BC_val_loss} - pde: {PDE_val_loss}\")\n",
    "            print(f\"  time : {end - start} s  - train: {end_tr - start_tr} s  - val: {end_val - start_val} s\")\n",
    "            print(f\"     ( w_IC : {weight_IC} - w_BC : {weight_BC} - w_pde : {weight_PDE} - lambda_eff : {lambda_eff} )\")\n",
    "            print(f\"     ( learning rate : {current_lr})\")\n",
    "            self.history.append([ IC_total_loss, PDE_total_loss, IC_val_loss, PDE_val_loss,\n",
    "                            weight_IC, weight_PDE, lambda_eff, current_lr, BC_total_loss, BC_val_loss, weight_BC ])\n",
    "            print(f\" \")\n",
    "            print(f\" \")\n",
    "\n",
    "\n",
    "\n",
    "            # PLOTs\n",
    "            if epoch % 100 == 0: # and epoch>0:\n",
    "                sampled_times = Norm_time( t_min + np.array([ 0, 0.1, 0.2, 0.3, 0.7, 1 ]) * (t_max - t_min) )\n",
    "                n_samples = 20000\n",
    "                plt.figure(figsize=(10,4))\n",
    "                self.eval()\n",
    "                for i_plot, n_t in enumerate(sampled_times, 1):\n",
    "                    real_t = Inv_Norm_time(n_t)\n",
    "                    X_pred = self._PDE.gen_PDE_points(batch_size=n_samples)\n",
    "                    X_pred[:,0] = n_t * torch.ones_like(X_pred[:,0], device=device)\n",
    "                    X_Sample = X_pred.detach().cpu().numpy()\n",
    "                    X_pred.requires_grad=True\n",
    "                    V_pred = model.forward(X_pred)\n",
    "                    \n",
    "                    E_pred = ( (self._PDE.get_derivative(V_pred, X_pred, 1)[:,1].unsqueeze(-1)/factor_z)**2 ) ** (1/2)\n",
    "                    V_pred = V_pred.detach().cpu().numpy()\n",
    "                    E_pred = E_pred.detach().cpu().numpy()\n",
    "\n",
    "                    v_min = 0\n",
    "                    v_max = V0\n",
    "                    plt.subplot(2, len(sampled_times), i_plot)\n",
    "                    vett = Inv_Norm_x(norm_x_min + (norm_x_max - norm_x_min) * torch.rand(len(X_Sample[:,0]), 1, device=device))\n",
    "                    vett = vett.detach().cpu().numpy()\n",
    "                    im0=plt.scatter(vett, Inv_Norm_z(X_Sample[:, 1]), c=V_pred, s=2, cmap='plasma', vmin=v_min, vmax=v_max)\n",
    "                    # linea rossa condensatore piastre\n",
    "                    x_vals = x_min + (x_max - x_min) * np.random.rand(1000)\n",
    "                    z_val  = z_max - L/2 + d/2\n",
    "                    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "                    z_val  = z_min+L/2 - d/2\n",
    "                    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "                    plt.title(f\"Q {real_t:.2f} ns\")\n",
    "                    plt.colorbar(im0)\n",
    "                    plt.tight_layout()\n",
    "\n",
    "                    v_min = 0\n",
    "                    v_max = 0.05\n",
    "                    plt.subplot(2, len(sampled_times), len(sampled_times) + i_plot)\n",
    "                    vett = Inv_Norm_x(norm_x_min + (norm_x_max - norm_x_min) * torch.rand(len(X_Sample[:,0]), 1, device=device))\n",
    "                    vett = vett.detach().cpu().numpy()\n",
    "                    im1=plt.scatter(vett, Inv_Norm_z(X_Sample[:, 1]), c=E_pred, s=2, cmap='plasma', vmin=v_min, vmax=v_max)\n",
    "                    # linea rossa condensatore piastre\n",
    "                    x_vals = x_min + (x_max - x_min) * np.random.rand(1000)\n",
    "                    z_val  = z_max - L/2 + d/2\n",
    "                    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "                    z_val  = z_min+L/2 - d/2\n",
    "                    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "                    plt.title(f\"E {real_t:.2f} ns\")\n",
    "                    plt.colorbar(im1)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # initial condition\n",
    "                points = self._PDE.gen_PDE_points(batch_size=10000)\n",
    "                points[:,0] = norm_t_min * torch.ones_like(points[:,0], device=device)\n",
    "                points.requires_grad=True\n",
    "                IC_values = get_IC_values(points)\n",
    "                IC_pred = self.forward(points)\n",
    "                E_pred = ( (self._PDE.get_derivative(IC_pred, points, 1)[:,1].unsqueeze(-1)/factor_z)**2 ) ** (1/2)\n",
    "                z_coords = points[:,1].detach().cpu().numpy()\n",
    "                IC_pred = IC_pred.detach().cpu().numpy()\n",
    "                IC_values = IC_values.detach().cpu().numpy()\n",
    "                E_pred = E_pred.detach().cpu().numpy()\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                axs[0].scatter(Inv_Norm_z(z_coords), IC_values, c=\"green\", s=1, label=\"IC_values\")\n",
    "                axs[0].scatter(Inv_Norm_z(z_coords), IC_pred, c=\"red\", s=1, label=\"IC_pred\")\n",
    "                axs[0].set_title(\"V Initial condition\")\n",
    "                axs[0].set_xlabel(\"z\")\n",
    "                axs[0].legend()\n",
    "                axs[1].scatter(Inv_Norm_z(z_coords), E_pred, c=\"red\", s=1, label=\"E_pred\")\n",
    "                axs[1].set_title(\"E Initial condition\")\n",
    "                axs[1].set_ylim(0,0.015)\n",
    "                axs[1].set_xlabel(\"z\")\n",
    "                axs[1].legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # final condition\n",
    "                points = self._PDE.gen_PDE_points(batch_size=10000)\n",
    "                points[:,0] = norm_t_max * torch.ones_like(points[:,0], device=device)\n",
    "                points.requires_grad=True\n",
    "                FC_pred = self.forward(points)\n",
    "                E_pred = ( (self._PDE.get_derivative(FC_pred, points, 1)[:,1].unsqueeze(-1) / factor_z)**2 ) ** (1/2)\n",
    "                z_coords = points[:,1].detach().cpu().numpy()\n",
    "                FC_pred = FC_pred.detach().cpu().numpy()\n",
    "                E_pred = E_pred.detach().cpu().numpy()\n",
    "                fig, axs = plt.subplots(1, 2, figsize=(12, 4))\n",
    "                axs[0].scatter(Inv_Norm_z(z_coords), FC_pred, c=\"red\", s=1, label=\"FC_pred\")\n",
    "                axs[0].set_title(\"V Final Condition\")\n",
    "                axs[0].set_ylim(0,1)\n",
    "                axs[0].set_xlabel(\"z\")\n",
    "                axs[0].legend()\n",
    "                axs[1].scatter(Inv_Norm_z(z_coords), E_pred, c=\"red\", s=1, label=\"E_pred\")\n",
    "                axs[1].set_title(\"E Final Condition\")\n",
    "                axs[1].set_xlabel(\"z\")\n",
    "                axs[1].set_ylim(0,0.05)\n",
    "                axs[1].legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "                # pde error\n",
    "                points = self._PDE.gen_PDE_points(batch_size=10000)\n",
    "                points.requires_grad=True\n",
    "                V_pred = self.forward(points)\n",
    "                pde_pred, _, _ = self._PDE.compute_PDE( points, V_pred )\n",
    "                z_coords = points[:,1].detach().cpu().numpy()\n",
    "                pde_pred = pde_pred.detach().cpu().numpy()\n",
    "                plt.scatter(Inv_Norm_z(z_coords), pde_pred, c=\"red\", s=1, label=\"pde error\")\n",
    "                plt.xlabel(\"z\")\n",
    "                plt.legend()\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        # END EPOCHS\n",
    "        self.history = torch.stack([torch.tensor(h) for h in self.history]).detach().cpu().numpy()\n",
    "        print(f\"Done!  (time : {time.time() - start_time})\")\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef06a56-e03e-41d7-a1d9-4e514741ae17",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab1247-b1c3-41c8-8e27-3721eebcfbd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### construction of PINN\n",
    "model = 0\n",
    "input_dim = 2     # (t, z)\n",
    "n_nodes = 64\n",
    "n_layers = 6\n",
    "n_batches = 32\n",
    "dropout = 0\n",
    "model = PINN_model( input_dim, n_nodes, n_layers, n_batches, dropout).to(device)\n",
    "\n",
    "# training of PINN\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-9)\n",
    "loss_function = nn.MSELoss()\n",
    "history = model.train_model(optimizer, patience=20, loss_function=loss_function, epochs=10000)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8371f9-2cd6-4ada-bce6-cfe3994f0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model_c1D.pth\")\n",
    "model.load_state_dict(torch.load(\"model_c1D.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063ca86d-14ab-406c-a2fe-7419fb991652",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history\n",
    "\n",
    "# PLOTS OF TRAINING AND VALIDATION STEPS\n",
    "# plot IC\n",
    "plt.plot([pair[0] for pair in history], label=\"Training\")\n",
    "plt.plot([pair[2] for pair in history], label=\"Validation\")\n",
    "plt.legend(title=\"IC error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss values (MSE)\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(\"plot_c1D_IC.jpg\", format=\"jpg\", dpi=300)\n",
    "plt.show()\n",
    "# plot BC\n",
    "plt.plot([pair[8] for pair in history], label=\"Training\")\n",
    "plt.plot([pair[9] for pair in history], label=\"Validation\")\n",
    "plt.legend(title=\"BC error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss values (MSE)\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(\"plot_c1D_BC.jpg\", format=\"jpg\", dpi=300)\n",
    "plt.show()\n",
    "# plot PDE CONDITIONS\n",
    "plt.plot([np.maximum(pair[1]/1e3, 1e-10) for pair in history], label=\"Training\")\n",
    "plt.plot([np.maximum(pair[3]/1e3, 1e-10) for pair in history], label=\"Validation\")\n",
    "plt.legend(title=\"PDE error\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss values (MSE)\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(\"plot_c1D_PDE.jpg\", format=\"jpg\", dpi=300)\n",
    "plt.show()\n",
    "# plot LAMBDA EFFECTIVE\n",
    "plt.plot([pair[6] for pair in history], label=\"lambda eff\")\n",
    "plt.legend(title=\"Lambda effective\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"values\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.yscale(\"log\")\n",
    "plt.show()\n",
    "# plot LEARNING RATE\n",
    "plt.plot([pair[7] for pair in history], label=\"lr\")\n",
    "plt.legend(title=\"Learning rate\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"values\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de61fb-0f44-4d56-b82d-11bc54737cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTIONs\n",
    "sampled_times = Norm_time( t_min + np.array([ 0, 0.1, 0.3, 0.7, 1, 1.5 ]) * (t_max - t_min) )\n",
    "n_samples = 20000\n",
    "plt.figure(figsize=(16,14))\n",
    "model.eval()\n",
    "sigma_model.eval()\n",
    "pde_class = QSM_PDE()\n",
    "\n",
    "for i_plot, n_t in enumerate(sampled_times, 1):\n",
    "    real_t = Inv_Norm_time(n_t)\n",
    "    X_pred = pde_class.gen_PDE_points(batch_size=n_samples)\n",
    "    X_pred[:,0] = n_t * torch.ones_like(X_pred[:,0], device=device)\n",
    "    X_Sample = X_pred.detach().cpu().numpy()\n",
    "    X_pred.requires_grad=True\n",
    "    V_pred = model.forward(X_pred)\n",
    "    \n",
    "    E_pred = ( (pde_class.get_derivative(V_pred, X_pred, 1)[:,1].unsqueeze(-1) / factor_z)**2 ) ** (1/2)\n",
    "    V_pred = V_pred.detach().cpu().numpy()\n",
    "    E_pred = E_pred.detach().cpu().numpy()\n",
    "\n",
    "    v_min = 0\n",
    "    v_max = V0\n",
    "    plt.subplot(4, len(sampled_times), i_plot)\n",
    "    vett = Inv_Norm_x(norm_x_min + (norm_x_max - norm_x_min) * torch.rand(len(X_Sample[:,0]), 1, device=device))\n",
    "    vett = vett.detach().cpu().numpy()\n",
    "    im0=plt.scatter(vett, Inv_Norm_z(X_Sample[:, 1]), c=V_pred, s=2, cmap='plasma', vmin=0, vmax=V0)\n",
    "    # linea rossa condensatore piastre\n",
    "    x_vals = x_min + (x_max - x_min) * np.random.rand(1000)\n",
    "    z_val  = z_max - L/2 + d/2\n",
    "    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "    z_val  = z_min + L/2 - d/2\n",
    "    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "    plt.title(f\"V(t={real_t:.2f} ns)\")\n",
    "    plt.xlabel(\"x coordinate [$\\\\mu$m]\")\n",
    "    if i_plot == 1:\n",
    "        plt.ylabel(\"z coordinate [$\\\\mu$m]\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(z_min, z_max)\n",
    "    plt.colorbar(im0)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(4, len(sampled_times), len(sampled_times) + i_plot)\n",
    "    im2=plt.scatter(Inv_Norm_z(X_Sample[:, 1]), V_pred, s=1)\n",
    "    plt.title(f\"t = {real_t:.2f} ns\")\n",
    "    if i_plot == 1:\n",
    "        plt.ylabel(\"Potential [V]\")\n",
    "    plt.xlabel(\"z coordinate [$\\\\mu$m]\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    v_min = 0\n",
    "    v_max = (V0 - 0) / ( norm_z_max - norm_z_min ) / factor_z\n",
    "    plt.subplot(4, len(sampled_times), 2*len(sampled_times) + i_plot)\n",
    "    vett = Inv_Norm_x(norm_x_min + (norm_x_max - norm_x_min) * torch.rand(len(X_Sample[:,0]), 1, device=device))\n",
    "    vett = vett.detach().cpu().numpy()\n",
    "    im1=plt.scatter(vett, Inv_Norm_z(X_Sample[:, 1]), c=E_pred, s=2, cmap='plasma', vmin=0, vmax=0.06)\n",
    "    # linea rossa condensatore piastre\n",
    "    x_vals = x_min + (x_max - x_min) * np.random.rand(1000)\n",
    "    z_val  = z_max - L/2 + d/2\n",
    "    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "    z_val  = z_min + L/2 - d/2\n",
    "    plt.scatter(x_vals, np.full_like(x_vals, z_val), c=\"red\", s=1)\n",
    "    plt.title(f\"E(t={real_t:.2f} ns)\")\n",
    "    plt.xlabel(\"x coordinate [$\\\\mu$m]\")\n",
    "    if i_plot == 1:\n",
    "        plt.ylabel(\"z coordinate [$\\\\mu$m]\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(z_min, z_max)\n",
    "    plt.colorbar(im1)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.subplot(4, len(sampled_times), 3*len(sampled_times) + i_plot)\n",
    "    im3=plt.scatter(Inv_Norm_z(X_Sample[:, 1]), E_pred, s=1)\n",
    "    plt.ylim(0,0.06)\n",
    "    plt.title(f\"t = {real_t:.2f} ns\")\n",
    "    if i_plot == 1:\n",
    "        plt.ylabel(\"Electric field [V/$\\\\mu$m]\")\n",
    "    plt.xlabel(\"z coordinate [$\\\\mu$m]\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"plot_c1D.jpg\", format=\"jpg\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nVidia Modulus",
   "language": "python",
   "name": "modulus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
